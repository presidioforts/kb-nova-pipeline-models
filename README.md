# KB Nova Pipeline Models

A comprehensive AI/ML pipeline for knowledge base processing and model training with production-ready folder structure and security standards.

## ğŸ—ï¸ Project Structure

```
kb-nova-pipeline-models/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ data/                     # Data processing modules
â”‚   â”œâ”€â”€ models/                   # Model definitions and architectures
â”‚   â”œâ”€â”€ features/                 # Feature engineering
â”‚   â”œâ”€â”€ visualization/            # Data visualization utilities
â”‚   â”œâ”€â”€ utils/                    # Utility functions
â”‚   â”œâ”€â”€ api/                      # API endpoints and services
â”‚   â”œâ”€â”€ training/                 # Model training scripts
â”‚   â”œâ”€â”€ inference/                # Model inference and prediction
â”‚   â””â”€â”€ evaluation/               # Model evaluation and metrics
â”œâ”€â”€ data/                         # Data storage
â”‚   â”œâ”€â”€ raw/                      # Raw, immutable data
â”‚   â”œâ”€â”€ processed/                # Cleaned and processed data
â”‚   â”œâ”€â”€ external/                 # External data sources
â”‚   â””â”€â”€ interim/                  # Intermediate data transformations
â”œâ”€â”€ models/                       # Trained models and artifacts
â”‚   â”œâ”€â”€ trained/                  # Serialized trained models
â”‚   â””â”€â”€ artifacts/                # Model artifacts and metadata
â”œâ”€â”€ notebooks/                    # Jupyter notebooks
â”‚   â”œâ”€â”€ exploratory/              # Exploratory data analysis
â”‚   â””â”€â”€ experiments/              # Model experiments
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ unit/                     # Unit tests
â”‚   â””â”€â”€ integration/              # Integration tests
â”œâ”€â”€ configs/                      # Configuration files
â”œâ”€â”€ scripts/                      # Utility scripts
â”œâ”€â”€ docs/                         # Documentation
â”œâ”€â”€ logs/                         # Application logs
â”œâ”€â”€ reports/                      # Generated reports
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ setup.py                      # Package setup
â”œâ”€â”€ pyproject.toml               # Modern Python project config
â”œâ”€â”€ Makefile                     # Development automation
â””â”€â”€ .gitignore                   # Git ignore rules
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- Git
- Virtual environment (recommended)

### Installation

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd kb-nova-pipeline-models
   ```

2. **Set up virtual environment:**
   ```bash
   python -m venv venv
   
   # Windows
   venv\Scripts\activate
   
   # Unix/MacOS
   source venv/bin/activate
   ```

3. **Install dependencies:**
   ```bash
   make install-dev
   # or
   pip install -r requirements.txt
   ```

4. **Set up environment:**
   ```bash
   cp env.example .env
   # Edit .env with your configuration
   ```

5. **Run setup script:**
   ```bash
   python scripts/setup.py
   ```

## ğŸ› ï¸ Development

### Available Commands

```bash
make help                 # Show all available commands
make install             # Install production dependencies
make install-dev         # Install development dependencies
make test                # Run tests
make lint                # Run linting checks
make format              # Format code
make security            # Run security checks
make clean               # Clean up temporary files
make train               # Train models
make predict             # Run predictions
make serve               # Start API server
make docs                # Generate documentation
```

### Code Quality

This project enforces high code quality standards:

- **Black** for code formatting
- **Flake8** for linting
- **MyPy** for type checking
- **Bandit** for security analysis
- **Pytest** for testing with coverage
- **Pre-commit hooks** for automated checks

### Testing

```bash
# Run all tests
make test

# Run specific test types
make test-unit
make test-integration

# Run with coverage
pytest --cov=src --cov-report=html
```

## ğŸ“Š ML Pipeline

### Data Processing

```bash
# Process raw data
make process-data
# or
python -m src.data.process
```

### Model Training

```bash
# Train models
make train
# or
python -m src.training.train
```

### Model Evaluation

```bash
# Evaluate models
make evaluate
# or
python -m src.evaluation.evaluate
```

### Inference

```bash
# Run predictions
make predict
# or
python -m src.inference.predict
```

## ğŸŒ API

Start the API server:

```bash
# Development
make serve

# Production
make serve-prod
```

The API will be available at `http://localhost:8000`

### Chroma DB Integration

This project integrates with [Chroma](https://www.trychroma.com/), an open-source AI application database that provides:

- **Vector embeddings** using your fine-tuned SentenceTransformer models
- **Semantic search** across knowledge base items and chat history
- **Persistent storage** for conversations and documents
- **Metadata filtering** for advanced queries
- **Multi-modal support** for future enhancements

#### Key Features:
- ğŸ” **Semantic Search**: Find relevant solutions using vector similarity
- ğŸ’¬ **Context-Aware Chat**: Conversations that remember previous interactions
- ğŸ“š **Knowledge Base Integration**: Automatic indexing of troubleshooting solutions
- ğŸ”„ **Real-time Updates**: Dynamic addition of new knowledge and conversations
- ğŸ“Š **Analytics**: Track usage patterns and improve responses

#### Chat API Endpoints:
- `POST /chat` - Send a chat message with context awareness
- `POST /chat/session` - Create a new chat session
- `GET /chat/session/{id}/history` - Get chat history
- `GET /chat/search` - Search across conversations
- `GET /chroma/stats` - Get Chroma database statistics

#### Demo:
```bash
# Run the Chroma integration demo
python examples/chroma_demo.py
```

#### Learn More:
- ğŸŒ **Chroma Official Website**: [https://www.trychroma.com/](https://www.trychroma.com/)
- ğŸ“– **Chroma Documentation**: Available on their website
- ğŸ’¬ **Chroma Community**: Join their Discord for support

## ğŸ“ˆ MLOps

### Experiment Tracking

- **MLflow**: Track experiments and model versions
- **Weights & Biases**: Advanced experiment tracking
- **TensorBoard**: Visualization of training metrics

```bash
# Start MLflow UI
make mlflow-ui
```

### Model Registry

Models are automatically registered and versioned using MLflow.

## ğŸ”’ Security

This project follows security best practices:

- Environment variables for sensitive data
- Security scanning with Bandit
- Dependency vulnerability checking
- Secure defaults in configurations
- Input validation and sanitization

## ğŸ“ Configuration

Configuration is managed through:

- `configs/config.yaml` - Main configuration
- `.env` - Environment variables
- `pyproject.toml` - Tool configurations

## ğŸ§ª Jupyter Notebooks

```bash
# Start Jupyter Lab
make jupyter
```

Notebooks are organized in:
- `notebooks/exploratory/` - Data exploration
- `notebooks/experiments/` - Model experiments

## ğŸ“š Documentation

Generate and serve documentation:

```bash
make docs
make docs-serve
```

## ğŸ³ Docker (Optional)

```bash
# Build Docker image
make docker-build

# Run container
make docker-run
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests and linting
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Support

For support and questions:

- Create an issue in the repository
- Check the documentation in `docs/`
- Review the configuration files in `configs/`

##### Tags: ml-models, transformers, pipeline, rag, ai, data-science, mlops
